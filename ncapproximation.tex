\documentclass[]{article}

% Package `hyperref` must come before package `complexity`.
\usepackage[pdftitle={Definitions and reductions for NC approximation problems}, pdfauthor={Jeffrey Finkelstein}]{hyperref}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{complexity}
\usepackage{tikz}

\theoremstyle{plain}
\newtheorem{conjecture}{Conjecture}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{todo}{TODO}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{openquestion}{Open question}

\newenvironment{justification}{\begin{proof}[Justification]}{\end{proof}}

\newcommand{\definitionautorefname}{Definition}

\newcommand{\Er}{\leq_E^{L}}
\newcommand{\APr}{\leq_{AP}^{L}}
\newcommand{\email}[1]{\href{mailto:#1}{\nolinkurl{#1}}}
\newcommand{\lb}{\left\{}
\newcommand{\pb}{\textsf{pb}}
\newcommand{\rb}{\right\}}
\newcommand{\st}{\,\middle|\,}
\newcommand{\cl}{\operatorname{cl}}

\newcommand{\nncoproblems}{%
  \textsc{Maximum Variable Weighted Satisfiability} \cite[Theorem~3.1]{om87} \cite[Theorem~8.3]{acgkmp99}}
\newcommand{\expapxncoproblems}{}
\newcommand{\polyapxncoproblems}{}
\newcommand{\logapxncoproblems}{}
\newcommand{\apxncoproblems}{%
  \textsc{Maximum $k$-CNF Satisfiability} \cite[Theorem~8.6]{acgkmp99},
  \textsc{Maximum Acyclic Subgraph} \cite[Section~7.4]{dsst97},
  \textsc{Minimum $k$-Center} \cite[Section~7.4]{dsst97},
  \textsc{$k$-Switching Network} \cite[Section~7.4]{dsst97}, and
  \textsc{Maximum Bounded Weighted Satisfiability} \cite[Theorem~4]{sx95}}
\newcommand{\apxncopproblems}{%
  \textsc{Induced Subgraph of High Weight for Linear Extremal Properties} \cite{dsst97}}
\newcommand{\rncasproblems}{%
  \textsc{Minimum Metric Traveling Salesperson} \cite[Theorem~7.1.1]{dsst97}}
\newcommand{\ncasproblems}{%
  \textsc{Maximum $k$-Constraint Satisfaction Problem} \cite[Corollary~13]{trevisan98}, and
  \textsc{Maximum Independent Set for Planar Graphs} \cite[Theorem 6.4.1]{dsst97}}
\newcommand{\ncaspproblems}{%
  \textsc{Maximum Matching} \cite[Theorem~5.2.1]{dsst97},
  \textsc{Maximum Weight Matching} \cite[Theorem~5.2.2]{dsst97},
  \textsc{Positive Linear Programming} \cite[Theorem~5.1.11]{dsst97} \cite{tx98}, and
  \textsc{Maximum Flow} \cite[Theorem~5.2.2]{dsst97}}
\newcommand{\frncaspproblems}{%
  \textsc{Maximum Flow} \cite[Theorem~4.5.2]{dsst97},
  \textsc{Maximum Weight Perfect Matching} \cite[Theorem~4.5.2]{dsst97}, and
  \textsc{Maximum Weight Matching} \cite[Theorem~4.5.2]{dsst97}}
\newcommand{\fncasproblems}{%
  \textsc{Subset Sum} \cite[Theorem~4.1.4]{dsst97},
  \textsc{Maximum Clause Weighted CNF Satisfiability} \cite[Theorem~8]{trevisan98},
  \textsc{Minimum Weight Vertex Cover} \cite[Theorem~5.3.6]{dsst97},
  \textsc{0-1 Knapsack} \cite[Theorem~2]{mayr88}, and
  \textsc{Bin Packing} \cite[Theorem~3]{mayr88}}
\newcommand{\ncoproblems}{}

\author{Jef{}frey Finkelstein}
\date{\today}
\title{Definitions and reductions for \texorpdfstring{\NC}{NC} approximation problems}

\begin{document}

\maketitle

\section{Introduction}

\NC{} is the class of computational problems decidable by a logarithmic space uniform family of Boolean circuits of bounded fan-in, polynomial size, and polylogarithmic depth.
Such problems are considered both ``efficient'' (since $\NC \subseteq \P$) and ``highly parallel'' (since we might consider each gate in the circuit to be a processor working in parallel and the small depth of the circuit a small number of steps).
By contrast, problems which are \P-complete (under logarithmic space or even \NC{} many-one reductions) are considered ``inherently sequential''.
Furthermore, all \NP-hard and \PSPACE-hard problems are also inherently sequential, since $\P \subseteq \NP \subseteq \PSPACE$.
Just as we hope to find efficient approximation algorithms for optimization problems for which it is intractable to compute an exact solution, so too do we hope to find efficient and highly parallel approximation algorithms for optimization problems for which computing the exact solution is inherently sequential.
(However, just like hardness of approximation for \NP-hard problems, in some cases even \emph{approximating} a solution inherently sequential!)

\section{Definitions for \texorpdfstring{\NC}{NC} approximation classes}

Throughout this work, $\Sigma=\{0, 1\}$ and inputs and outputs are encoded in binary.
The set of all finite strings is denoted $\Sigma^*$, and for each $x\in\Sigma^*$, we denote the length of $x$ by $|x|$.
We denote the set of all polynomials by \poly{} and the set of all polylogarithmic functions by \polylog.

\subsection{Optimization problems, approximators, and approximation schemes}

We adapt the clear and concise definitions of \cite{tantau07} from logarithmic space approximability to \NC{} approximability.

\begin{definition}[\cite{acgkmp99}]
  An \emph{optimization problem} is a four-tuple, $(I, S, m, t)$, where the set $I \subseteq \Sigma^*$ is called the \emph{instance set}, the set $S \subseteq I \times \Sigma^*$ is called the \emph{solution relation}, the function $m \colon S \to \mathbb{N}^+$ is called the \emph{measure function}, and $t \in \{\min, \max\}$ is called the \emph{type} of the optimization.
\end{definition}

\begin{definition}[\cite{tantau07}]
  Let $P$ be an optimization problem, so $P = (I, S, m, t)$, and let $x\in I$.
  \begin{enumerate}
  \item Let $S(x)=\lb y\in\Sigma^* \st (x, y)\in S \rb$; we call this the \emph{solutions for $x$}.
  \item Define $m^*(x)$ by
    \begin{displaymath}
      m^*(x) =
      \begin{cases}
        \min \lb m(x, y) \st y\in S(x) \rb & \text{if } t = \min \\
        \max \lb m(x, y) \st y\in S(x) \rb & \text{if } t = \max
      \end{cases}
    \end{displaymath}
    for all $x\in \Sigma^*$; we call this the \emph{optimal measure for $x$}.
    Let $m^*(x)$ be undefined if $S(x)=\emptyset$.
  \item Let $S^*(x)=\lb y\in\Sigma^* \st m(x, y) = m^*(x) \rb$; we call this the \emph{set of optimal solutions for $x$}.
  \item Let $R(x, y)=\max\left( \frac{m(x, y)}{m^*(x)}, \frac{m^*(x)}{m(x, y)} \right)$; we call this the \emph{performance ratio of the solution $y$}.
  \item Let $P_\exists = \lb x\in \Sigma^* \st S(x) \neq \emptyset \rb$; we call this the \emph{existence problem}.
  \item Let
    \begin{displaymath}
      P_{opt<}=\lb (x, z) \in P_\exists\times\mathbb{N} \st \exists y\in\Sigma^*\colon m(x, y) < z \rb
    \end{displaymath}
    and
    \begin{displaymath}
      P_{opt>}=\lb (x, z) \in P_\exists\times\mathbb{N} \st \exists y\in\Sigma^*\colon m(x, y) > z \rb;
    \end{displaymath}
    call these the \emph{budget problems}.
  \item Let $f\colon \Sigma^*\to\Sigma^*$.
    We say \emph{$f$ produces solutions for $P$} if for all $x\in P_\exists$ we have $f(x)\in S(x)$.
    We say \emph{$f$ produces optimal solutions for $P$} if for all $x\in P_\exists$ we have $f(x)\in S^*(x)$.
  \end{enumerate}
\end{definition}

The performance ratio $R(x, y)$ is a number in the interval $[1, \infty)$.
The closer $R(x, y)$ is to 1, the better the solution $y$ is for $x$, and the closer $R(x, y)$ to $\infty$, the worse the solution.

\begin{definition}
  Let $P$ be an optimization problem, let $r\colon \mathbb{N}\to\mathbb{N}$, and let $f\colon I\to \Sigma^*$.
  We say $f$ is an \emph{$r$-approximator for $P$} if it produces solutions for $P$ and $R(x, f(x)) \leq r(|x|)$ for all $x\in P_\exists$.

  If $r$ is the constant function with value $\delta$, we simply say $f$ is a \emph{$\delta$-approximator for $P$}.
\end{definition}

\begin{definition}
  Let $P$ be an optimization problem and let $f\colon I\times\mathbb{N}\to\Sigma^*$.
  We say $f$ is an \emph{approximation scheme for $P$} if for all $x\in P_\exists$ and all positive integers $k$ we have $f(x, k)\in S(x)$ and $R(x, f(x, k)) \leq 1 + \frac{1}{k}$.
\end{definition}

\subsection{Classes of optimization problems}

The study of \emph{efficient} approximations for \emph{intractable} problems begins with the following definition of \NP{} optimization problems.
We will adapt this definition to explore \emph{efficient and highly parallel} approximations for \emph{inherently sequential} problems.

\begin{definition}\label{def:npo}
  The complexity class \NPO{} is the class of all optimization problems $(I, S, m, t)$ such that the following conditions hold.
  \begin{enumerate}
  \item The instance set $I$ is decidable by a deterministic polynomial time Turing machine.
  \item The solution relation $S$ is decidable by a deterministic polynomial time Turing machine and is polynomially bounded (that is, the length of $y$ is bounded by a polynomial in the length of $x$ for all $(x, y)\in S$).
  \item The measure function $m$ is computable by a deterministic polynomial time Turing machine.
  \end{enumerate}
\end{definition}

The second condition is the most important in this definition; it is the analog of polynomial time verifiability in \NP.

\begin{definition}
  The complexity class \PO{} is the subclass of \NPO{} in which for each optimization problem $P$ there exists a function $f$ in \FP{} that produces optimal solutions for $P$.
\end{definition}

We now wish to translate these definitions to the setting of efficient and highly parallel verifiability.
In order to take advantage of results and techniques from the study of \NPO{} and \PO, we will start by considering the nondeterministic analog of \PO.
First we define the necessary circuit classes, then we define the corresponding classes of approximation problems.

\begin{definition}
  \mbox{}
  \begin{enumerate}
  \item \NC{} is the class of decision problems decidable by a logarithmic space uniform family of Boolean circuits with polynomial size, polylogarithmic depth, and fan-in two.
  \item \FNC{} is the class of functions computable by an \NC{} circuit in which the output of the circuit is (the binary encoding of) $f(x)$.
  \item $\NNC(f(n))$ is the class of languages computable by an \NC{} circuit family augmented with $O(f(n))$ nondeterministic gates for each input length $n$ \cite{wolf94}.
    A nondeterministic gate takes no inputs and yields a single (nondeterministic) output bit.

    If $\mathcal{F}$ is a class of functions, then $\NNC(\mathcal{F})=\bigcup_{f\in\mathcal{F}}{\NNC(f(n))}$.
  \end{enumerate}
\end{definition}

\NNCpoly, also known as \GC(\poly, \NC) \cite{cc97} and $\beta\P$ \cite{kf80}, is an unusual class which may warrant some further explanation.
\NC{} has the same relationship to \NNCpoly{} as \P{} does to \NP{} (thus an equivalent definition of \NNCpoly{} is one in which each language has an efficient and highly parallel verification procedure; as in the definition of \NPO{} in \autoref{def:npo}, it is this formulation which we use when defining \NNCO{} in \autoref{def:nnco}).
Wolf \cite{wolf94} notes that $\NNC(\log n)=\NC$ and $\NNC(\poly)=\NP$, and suggests that \NNC(\polylog) may be an interesting intermediary class, possibly incomparable with \P.
Cai and Chen \cite{cc97} prove that for each natural number $k$ and $i$, there is a complete problem for $\NNC^k(\log^i n)$ under logarithmic space many-one reductions.

\begin{definition}\label{def:nnco}
  The complexity class \NNCOpoly{} is the class of all optimization problems $(I, S, m, t)$ such that the following conditions hold.
  \begin{enumerate}
  \item The instance set $I$ is decidable by an \NC{} circuit family.
  \item The solution relation $S$ is decidable by an \NC{} circuit family and is polynomially bounded (that is, the length of $y$ is bounded by a polynomial in the length of $x$ for all $(x, y)\in S$).
  \item The measure function $m$ is computable by an \FNC{} circuit family.
  \end{enumerate}
  For the sake of brevity, we write \NNCO{} instead of \NNCOpoly{}.
\end{definition}

Since $\NC\subseteq\P$, the inclusion $\NNCO\subseteq\NPO$ follows immediately from the definitions.
However, these definitions do not imply that the converse is true, since membership in \P{} does not imply membership in \NC.
In fact, there is evidence that $\NNCO \neq \NPO$.
\begin{proposition}[Thanks to Sam for this one.]
  $\NNCO = \NPO$ if and only if $\P = \NC$.
\end{proposition}
\begin{proof}
  We know $\NNCO \subseteq \NPO$ unconditionally from the preceding paragraph.
  If we suppose $\P = \NC$, then it follows immediately from the definitions that any optimization problem in $\NPO$ is also in $\NNCO$.

  Now suppose $\NNCO = \NPO$.
  If $C$ is a Boolean circuit with a single output, let $C(x)$ denote the output of $C$ (as a bit).
  Consider the optimization problem $Q$ defined by $Q = (I, S, m, t)$, where
  \begin{align*}
    I & = \{ C \, | \, C \text{ is a Boolean circuit} \}, \\
    S & = \{ (C, x) \, | \, C(x) = 1 \}, \\
    m(C, x) & = C(x), \text{ and} \\
    t & = \max.
  \end{align*}
  (Observe that $Q$ is an optimization form of the \textsc{Circuit Satisfiability} problem, where we have $m(C, x) \in \{0, 1\}$ for all circuits $C$ and all strings $x$.)
  $I \in \P$ because a correctly formatted Boolean circuit is a directed acyclic graph, and a depth first search (computable in deterministic polynomial time) can reveal if a given directed graph has a cycle.
  $S \in \P$ because it is exactly the \textsc{Circuit Value} problem, which is known to be in \P{} (in fact, it is \P-complete \cite[Problem~A.1.1]{ghr95}).
  $m \in \P$ because it is simply the function form of the \textsc{Circuit Value} problem.
  Therefore, $Q \in \NPO$.

  Since $\NNCO = \NPO$ by hypothesis, $Q \in \NNCO$.
  Hence, $S \in \NC$.
  The existence of an \NC{} algorithm for a \P-complete problem implies $\P = \NC$.
  Therefore, $\NNCO = \NPO$ if and only if $\P = \NC$.
\end{proof}

We can now proceed to define classes of approximable optimization problems contained in \NNCO.

\begin{definition}\label{def:ncx}
  Let $P\in\NNCO$.
  \begin{enumerate}
  \item $P\in\expApxNCO$ if there exists an $r$-approximator in \FNC{} for $P$, where $r(n)=2^{n^{O(1)}}$ for all $n\in\mathbb{N}$.
  \item $P\in\polyApxNCO$ if there exists an $r$-approximator in \FNC{} for $P$, where $r(n)=n^{O(1)}$ for all $n\in\mathbb{N}$.
  \item $P\in\logApxNCO$ if there exists an $r$-approxiator in \FNC{} for $P$, where $r(n)=O(\log n)$ for all $n\in\mathbb{N}$.
  \item $P\in\ApxNCO$ if there exists an $r$-approximator in \FNC{} for $P$, where $r(n)=O(1)$ for all $n\in\mathbb{N}$.
    (This class is also known as \NCX.)
  \item $P\in\NCAS$ if there exists an approximation scheme $f$ for $P$ such that $f_k\in\FNC$ for each $k\in\mathbb{N}$, where $f_k(x)=f(x, k)$ for all $x\in\Sigma^*$.
  \item $P\in\FNCAS$ if there exists an approximation scheme $f$ for $P$ such that $f\in\FNC$ in the sense that the size of the circuit is polynomial in both $|x|$ and $k$ and the depth of the circuit is polylogarithmic in both $|x|$ and $k$.
  \item $P\in\NCO$ if there exists a function $f$ in \FNC{} that produces optimal solutions for $P$.
  \end{enumerate}
\end{definition}

Each of these classes includes the one defined below it.

Each class also has a ``polynomially bounded'' variant, denoted by the subscript \pb, in which an optimization problem with measure $m$ and solution set $S$ has the property that there exists a polynomial $p$ such that $m(x, y) \leq p(|x|)$ for all $(x, y)\in S$.
Another common variant of these classes is the probabilistic versions of each (for example, randomized \NCAS{}, denoted \RNCAS), but we do not explore such classes in this work.

\begin{todo}
  How does \PO{} compare to the \NC{} approximation classes?
  \cite[Figure~2.2]{dsst97} suggests that \P{} is incomparable to each of \FNCAS, \NCAS, and \ApxNCO, but provides no other explanation.
\end{todo}

The chain of inclusions given in \autoref{def:ncx} provides a hierarchy of classes which classify approximability of problems in \NNCO{}, and hence in \NPO.
However, our intention is to determine the approximability of optimization problems corresponding to \P-complete decision problems, not those corresponding to \NP-complete decision problems.
Therefore we define the following subclasses of \PO{} in order to more accurately capture the notion of highly efficient approximability of inherently sequential problems.

\begin{definition}\label{def:poprime}
  The complexity class \POp{} is the class of all optimization problems $(I, S, m, t)$ such that the following conditions hold.
  \begin{enumerate}
  \item The instance set $I$ is decidable by an \NC{} circuit family.
  \item The solution relation $S$ is decidable by an \NC{} circuit family and is polynomially bounded (that is, the length of $y$ is bounded by a polynomial in the length of $x$ for all $(x, y)\in S$).
  \item The measure function $m$ is computable by an \FNC{} circuit family.
  \item There exists a function $f$ in \FP{} that produces optimal solutions for $P$.
  \end{enumerate}
\end{definition}

\begin{definition}\label{def:ncxprime}
  Let $P\in\POp$.
  \begin{enumerate}
  \item $P\in\expApxNCOp$ if there exists an $r$-approximator in \FNC{} for $P$, where $r(n)=2^{n^{O(1)}}$ for all $n\in\mathbb{N}$.
  \item $P\in\polyApxNCOp$ if there exists an $r$-approximator in \FNC{} for $P$, where $r(n)=n^{O(1)}$ for all $n\in\mathbb{N}$.
  \item $P\in\logApxNCOp$ if there exists an $r$-approxiator in \FNC{} for $P$, where $r(n)=O(\log n)$ for all $n\in\mathbb{N}$.
  \item $P\in\ApxNCOp$ if there exists an $r$-approximator in \FNC{} for $P$, where $r(n)=O(1)$ for all $n\in\mathbb{N}$.
    (This class is also known as \NCX.)
  \item $P\in\NCASp$ if there exists an approximation scheme $f$ for $P$ such that $f_k\in\FNC$ for each $k\in\mathbb{N}$, where $f_k(x)=f(x, k)$ for all $x\in\Sigma^*$.
  \item $P\in\FNCASp$ if there exists an approximation scheme $f$ for $P$ such that $f\in\FNC$ in the sense that the size of the circuit is polynomial in both $|x|$ and $k$ and the depth of the circuit is polylogarithmic in both $|x|$ and $k$.
  \end{enumerate}
\end{definition}

Observe that $\POp = \PO \cap \NNCO$ by definition, and similarly $\ApxNCOp = \PO \cap \ApxNCO$, etc.
\autoref{fig:inclusions} shows the inclusions among some of the complexity classes defined in this section.
\begin{figure}
  \caption{%
    Inclusions among some classes of approximable optimization problems.
    The complexity class at the tail of each arrow is a subset of the complexity class at the head of the arrow.
    \label{fig:inclusions}}
  \begin{center}
    \begin{tikzpicture}

      \draw (0, 7) node(NPO) {\NPO};

      \draw (-2, 5) node(NNCO) {\NNCO};
      \draw (-2, 4) node(ApxNCO) {\ApxNCO};
      \draw (-2, 3) node(NCAS) {\NCAS};

      \draw (1, 6) node(ApxPO) {\ApxPO};
      \draw (1, 5) node(PTAS) {\PTAS};
      \draw (1, 4) node(PO) {\PO};
      \draw (0, 3) node(POp) {\POp};
      \draw (0, 2) node(ApxNCOp) {\ApxNCOp};
      \draw (0, 1) node(NCASp) {\NCASp};

      \draw (0, 0) node(NCO) {\NCO};

      \path[->]
      (NCO) edge (NCASp)
      (NCASp) edge (ApxNCOp)
      (ApxNCOp) edge (POp)
      (POp) edge (PO)
      (PO) edge (PTAS)
      (PTAS) edge (ApxPO)
      (ApxPO) edge (NPO)

      (NCAS) edge (ApxNCO)
      (ApxNCO) edge (NNCO)

      (NCASp) edge (NCAS)
      (ApxNCOp) edge (ApxNCO)
      (POp) edge (NNCO)

      (NCAS) edge (PTAS)
      (ApxNCO) edge (ApxPO)
      (NNCO) edge (NPO);
    \end{tikzpicture}
  \end{center}
\end{figure}

\section{Reductions among approximation problems}

There are many reductions for approximation problems; nine of them are defined in a survey paper by Crescenzi \cite{crescenzi97}, and there are more defined elsewhere.
We will use a logarithmic space-bounded version of the ``AP reduction'', considered by approximation experts to be the a reasonable reduction to use when constructing complete problems \cite[Section~2]{crescenzi97} \cite[Section~8.6]{acgkmp99}.

\begin{definition}{\cite[Definition~9]{ckst95}}
  Let $P$ and $Q$ be optimization problems in \NNCO, with $P=(I_P, S_P, m_p, t_P)$ and $Q=(I_Q, S_Q, m_Q, t_Q)$.
  We say \emph{$P$ AP reduces to $Q$} and write $P\APr Q$ if there exist functions $f\colon\Sigma^*\to\Sigma^*$ and $g\colon\Sigma^*\to\Sigma^*$, and there exists a constant $\alpha\in\mathbb{R}$ such that:
  \begin{enumerate}
  \item For all $x\in I_P$ and all $r > 1$, we have $f(x, r)\in I_Q$.
  \item For all $x\in I_P$, all $r > 1$, and all $y\in S_Q(f(x, r))$, we have $g(x, y, r)\in S_P(x)$.
  \item $f$ and $g$ are computable in logarithmic space for any fixed $r$.
  \item For all $x\in I_P$, all $r > 1$, and all $y\in S_Q(f(x, r))$, we have $R_Q(f(x, r), y) \leq r \implies R_P(x, g(x, y, r)) \leq 1 + \alpha(r - 1)$.
  \end{enumerate}
\end{definition}

We bound the AP reduction to logarithmic space instead of allowing it be a \FNC{} circuit family because 1.~the former notion of reduction is more restrictive and hence implies the latter, 2.~existing results on approximability and approximation classes use the former, and 3.~it eases analysis in some proofs.

\begin{proposition}
  Let $\mathcal{C}$ be one of the complexity classes \NCO, \NCAS, \ApxNCO, \logApxNCO, \polyApxNCO, \expApxNCO, or \NNCO.
  Suppose $P\in\NNCO$ and $Q\in\mathcal{C}$.
  If $P\APr Q$ then $P\in \mathcal{C}$.
  In other words, $\mathcal{C}$ is closed under $\APr$ reductions.
\end{proposition}
\begin{proof}
  \begin{todo}
    fill me in
  \end{todo}
\end{proof}

\section{Complexity of extant parallel approximations}

For a listing of somewhat contemporary results on efficient \emph{sequential} approximation, see \cite{compendium}.
The list here summarizes best known approximation upper bounds for optimization problems.
\begin{todo}
  This list \emph{assumes} that the instance set and solution set for each optimization problem are decidable in \NC{} and that the measure function is computable in \FNC{}.
  The complexity of the instance set, solution set, and measure function must be verified for each problem individually, which will take some time.
\end{todo}

\begin{itemize}
\item \NNCO{} contains \nncoproblems.
\item \expApxNCO{} contains \expapxncoproblems.
\item \polyApxNCO{} contains \polyapxncoproblems.
\item \logApxNCO{} contains \logapxncoproblems
\item \ApxNCO{} contains \apxncoproblems.
\item \ApxNCOp{} contains \apxncopproblems.
\item \NCAS{} contains \ncasproblems.
\item \NCASp{} contains \ncaspproblems.
\item \FNCAS{} contains \fncasproblems.
\item \NCO{} contains \ncoproblems.
\end{itemize}

\begin{todo}
  There is a problem here.

  First, \textsc{Maximum 3-CNF Satisfiability} is complete for \APX{} under polynomial time AP reductions \cite[Theorem~8.6]{acgkmp99}.
  It is also complete for \NCX{} under \NC{} AP reductions \cite{finkelstein13b}.

  Next, \textsc{Maximum Clause-Weighted CNF Satisfiability} has a fully \NC{} approximation scheme \cite[Theorem~8]{trevisan98}.
  Given an assignment to the variables, each disjunctive clause can be verified in constant time, so the solution set is decidable in \NC{}.
  The measure of an assignment, the sum of the weights of the satisfied clauses, can also be computed in \NC{}.
  Therefore, \textsc{Maximum Clause-Weighted CNF Satisfiability} is in \FNCAS.

  There is a trivial logarithmic space approximation preserving reduction from the former problem to the latter problem.
  (In fact the approximation is preserved exactly.)
  \FNCAS{} is closed under such an approximation preserving reduction.
  Therefore \textsc{Maximum 3-CNF Satisfiability} is in \FNCAS and hence $\NCX = \FNCAS$, which implies $\NC = \NP$.
\end{todo}

\section{Completeness for classes of problems admitting parallel approximations}

We wish to demonstrate complete problems for each class of optimization problems admitting parallel approximations.

\begin{definition}
  Let $\mathcal{C}$ be one of the complexity classes \NCO, \NCAS, \ApxNCO, \logApxNCO, \polyApxNCO, \expApxNCO, or \NNCO.
  An optimization problem $Q$ is \emph{$\mathcal{C}$-hard} if for all $P\in\mathcal{C}$, we have $P\APr Q$.
  If furthermore $Q\in\mathcal{C}$, we say it is $\mathcal{C}$-complete.
\end{definition}

\subsection{Completeness of optimization problems corresponding to \texorpdfstring{\NP}{NP}-complete problems}

We start with optimization problems whose corresponding budget problem is \NP-complete.
These problems may have highly parallel approximation algorithms, but do not admit polynomial time functions that produce optimal solutions unless $\P = \NP$.

\begin{theorem}
  \textsc{Maximum Variable Weighted Satisfiability} is complete for \NNCO{} under $\APr$ reductions.
\end{theorem}
\begin{proof}
  First, this problem is in \NNCO.
  The measure function is computable in \FNC{} because arithmetic and summation are both computable in \FNC{}.
  The solution set is decidable in \NC{} because Boolean formula evaluation is computable in \NC{} \cite{buss87}.

  The proof that shows the problem is hard for \NNCO{} is similar to the one that shows \textsc{Maximum Variable Weighted Satisfiability} is hard for \NPO{} \cite[Theorem~3.1]{om87} \cite[Theorem~8.3]{acgkmp99}.
  The only difference is that the underlying machines are \NC{} and \NNC{} machines instead of \P{} and \NP{} machines.
\end{proof}

\begin{conjecture}
  \textsc{Minimum Traveling Salesperson Problem} is complete for \expApxNCO{} under $\APr$ reductions.
\end{conjecture}
\begin{justification}
  \textsc{Minimum Traveling Salesperson Problem} is complete for \expApxPO{} under ``MPTAS reductions'' \cite[Corollary~1]{ep06}.
\end{justification}

\begin{conjecture}
  \textsc{Maximum Clique} is \polyApxNCO-complete.
\end{conjecture}
\begin{justification}
  \textsc{Maximum Clique} is complete for \polyApxPO{} under ``PTAS reductions'' \cite[Example~2.48]{cks01} \cite{kmsv99} \cite{ep10}.
\end{justification}

\begin{conjecture}
  \textsc{Minimum Set Cover} is \logApxNCO-complete.
\end{conjecture}
\begin{justification}
  \textsc{Minimum Set Cover} is complete for \logApxPO{} under ``MPTAS reductions'' \cite[Example~2.48]{cks01} \cite[Theorem~5]{ep06} \cite[Theorem~27]{ep10}.

  Also, \textsc{Minimum Set Cover} is complete for $\logApxPO_{pb}$ under ``E reductions''.
\end{justification}

In \cite{sx95}, the authors prove that \textsc{Maximum Bounded Weighted Satisfiability} is complete for \ApxNCO{} (there called ``\NCX'') under a very unrestrictive type of reduction called an ``\NCAS{} reduction''.
Unfortunately, the proof there is very sketchy.
Their proof adapts the technique of \cite{cp91} for constructing a canonical complete problem for \ApxPO{} (there called ``\APX'').

\begin{theorem}
  \textsc{Maximum Bounded Weighted Satisfiability} is complete for \ApxNCO{} under \NCAS{} reductions.
\end{theorem}

In \cite[Theorem~8.6]{acgkmp99}, the authors prove that \textsc{Maximum $k$-Satisfiability} is complete for \ApxPO{} under polynomial time \AP{} reductions.
We believe that this proof translates to the \NC{} setting; for more information, see \cite{finkelstein13b}.

\begin{theorem}{\cite{finkelstein13b}}
  \textsc{Maximum $k$-CNF Satisfiability} is complete for \ApxNCO{} under \NC{} AP reductions.
\end{theorem}

\begin{conjecture}
  \mbox{}
  \begin{enumerate}
  \item \textsc{Planar Maximum Satisfiability} is \NCAS-complete.
  \item \textsc{Planar Maximum Independent Set} is \NCAS-complete.
  \end{enumerate}
\end{conjecture}
\begin{justification}
  \mbox{}
  \begin{enumerate}
  \item \textsc{Planar Maximum Satisfiability} is in \PMPSAT, which is a subset of \PTAS{} and \emph{may} be a syntactic characterization of \PTAS{} \cite{km96}.
  \item \textsc{Planar Maximum Independent Set} is complete for \PTAS{} under ``FT reductions'' \cite[Theorem~6]{bep06}. \qedhere
  \end{enumerate}
\end{justification}

\begin{conjecture}
  \textsc{0-1 Knapsack} is \FNCAS-complete.
\end{conjecture}
\begin{justification}
  \textsc{0-1 Knapsack} is in $\FPTAS\setminus\PO$ \cite[Section~3.2]{ep10} and is in \FNCAS{} \cite[Theorem~2]{mayr88}.
\end{justification}

\subsection{Completeness of optimization problems corresponding to \texorpdfstring{\P}{P}-complete problems}

It is \P-complete to compute a $\rho$-approximation for \textsc{Minimum-cost Maximum Flow} where $\rho$ is a polynomial in the length of the input \cite{sw92}.
It seems that this is true for all polynomials $\rho$, perhaps implying that this problem is not in \polyApxNCO{}, but it is not totally clear from the proof.

\begin{theorem}
  All of the following problems are \P-complete for all $\epsilon > 1$.
  \begin{itemize}
  \item \textsc{$\epsilon$-DPATH} \cite[Theorem~1]{ss89}
  \item \textsc{$\epsilon$-DUNIT} \cite[Theorem~2]{ss89}
  \item \textsc{$\epsilon$-Circuit Depth of Ones} \cite{kl88}
  \item \textsc{$\epsilon$-DGEN} \cite[Theorem~4]{ss89}
  \item \textsc{$\epsilon$-Linear Programming} (for both solution approximation and value approximation) \cite{serna91}
  \item \textsc{$\epsilon$-Reliable Connectivity Problem for in-edges} \cite[Theorem~4]{kks91}
  \end{itemize}
\end{theorem}

\begin{conjecture}
  All of the following problems are complete for \polyApxNCOp{} under $\APr$ reductions.
  \begin{itemize}
  \item \textsc{Maximum DPATH}
  \item \textsc{Maximum DUNIT}
  \item \textsc{Maximum Circuit Depth of Ones}
  \item \textsc{Minimum Solution Distance Linear Programming}
  \item \textsc{Minimum Objective Distance Linear Programming}
  \end{itemize}
\end{conjecture}

\begin{conjecture}
  All of the following problems are complete for \logApxNCOp{} under $\APr$ reductions.
  \begin{itemize}
  \item \textsc{Maximum DGEN}
  \item \textsc{Maximum Reliable Connectivity Problem for in-edges}
  \end{itemize}
\end{conjecture}

\begin{theorem}[{\cite[{Theorem~4 and Theorem~5}]{am84}}]
  All of the following problems are in \NC{} for all $\epsilon > 2$ but \P-complete for all $\epsilon \in [1, 2)$.
  \begin{itemize}
  \item \textsc{$\epsilon$-High Degree Subgraph} \cite[Theorem~4 and Theorem~5]{am84}
  \item \textsc{$\epsilon$-High Connectivity Subgraph} \cite[Theorem~6]{ss89}
  \end{itemize}
\end{theorem}

\begin{conjecture}
  All of the following problems are complete for \ApxNCOp{} under $\APr$ reductions.
  \begin{itemize}
  \item \textsc{Maximum High Degree Subgraph}
  \item \textsc{Maximum High Connectivity Subgraph}
  \end{itemize}
\end{conjecture}

An optimization problem for which any constant-factor approximation is \P-complete implies that the optimization problem is not in \ApxNCO{} (unless $\P=\NC$).
However, it may still be the case that these \P-complete optimization problems are in \logApxNCO, \polyApxNCO, \expApxNCO, or \NNCO.

\begin{todo}
  Produce a \P-complete problem which is complete for \ApxNCO{} (and then for each of the other approximation classes) by considering a restriction of an \NP-complete problem and following the same proof.
\end{todo}

\begin{todo}
  Produce a \P-complete problem which is complete for \ApxNCO{} (and then for each of the other approximation classes) by considering a relaxation of an \NL-complete problem and following a similar proof (this will be harder because a logarithmic space-bounded Turing machine can be described by a small directed graph representing its configurations and transitions).
\end{todo}

\begin{todo}
  Suppose $Q$ is an optimization problem, so $Q=(I, S, m, t)$, and $\hat{Q}$ is the budget problem for $Q$ which corresponds to $t$.
  Is it true that if $Q$ is \PO-complete then $\hat{Q}$ is \P-complete?
  Is the converse true?
\end{todo}

\begin{todo}[Thanks to Rita for this one]
  Do there exist, for example, \ApxPO-complete optimization problems which are based on both \NP-complete problems and \PSPACE-complete problems?
  \STP-complete problems?
\end{todo}

\subsection{Syntactic characterization of \texorpdfstring{\ApxNCO}{ApxNCO}}

In this section, $\cl(\mathcal{C}, \leq)$ denotes the closure of the complexity class $\mathcal{C}$ under $\leq$ reductions.

In \cite{py91}, the authors introduce a wealth of problems which are complete under ``L reductions'' for \MaxSNP, the class of maximization problems in strict \NP{} (\SNP), which is a syntactic characterization of \NP.
Further work showed that $\cl(\MaxSNP, \leq^P_E) = \ApxPO_{pb}$ \cite[Theorem~1]{kmsv99}, and since $\cl(\ApxPO_{pb}, \leq^P_{PTAS}) = \ApxPO$ \cite{ct00}, we conclude $\cl(\MaxSNP, \leq^P_{PTAS}) = \ApxPO$ \cite{kmsv99}.
We also have $\cl(\MaxSNP, \leq^P_E) = \cl(\MaxNP, \leq^P_E)$ \cite[Theorem~2]{kmsv99} and \textsc{Maximum Satisfiability} is complete for \MaxNP{} under $\leq^P_E$ reductions.
Can these results translate to \NC{} in any way?

According to \cite[Theorem~9.1.3]{dsst97}, $\MaxSNP \subseteq \NCX$.
They also ask the provide the following open question: is there a (possibly weaker) reduction $\leq^?_?$ such that $\cl(\MaxSNP, \leq^?_?) = \NCX$?
It may help to know that if $P$ is complete for $\MaxSNP$ under $\leq^L_L$ reductions, then $P$ exhibits threshold behavior for constant factor \NC{} approximation algorithms \cite[Theorem~9]{sx95} (see also \cite[Theorem~9.2.3]{dsst97}).

\begin{todo}
  We know that $\FO[\log^{O(1)} n] = \NC$ \cite[Theorem~5.2]{immerman99}; can we use this to construct a syntactic definition of $\ApxNCO$?
  Can we more easily construct a complete problem using this characterization?
\end{todo}

\section{Translating hardness of approximation results to \texorpdfstring{\NC}{NC}}

The celebrated \PCP{} theorem gives a precise characterization of \NP{} in terms of probabilistic verifiers with bounded randomness and queries, specifically that $\NP = \PCP(\log n, 1)$ \cite{pcp}.
This characterization allows many hardness of approximation results for optimization problems in \NPO{} corresponding to \NP-complete decision problems.

\begin{todo}
  Can the same be done for \NNC?
  Is there a \PCP{} characterization for \NNC?
\end{todo}

We have some initial results in this line of research \cite{finkelstein13}.

\section{About this work}

Copyright 2012, 2013 Jef{}frey Finkelstein.

This work is licensed under the Creative Commons Attribution-ShareAlike License 3.0.
Visit \mbox{\url{https://creativecommons.org/licenses/by-sa/3.0/}} to view a copy of this license.

The \LaTeX{} markup which generated this document is available on the World Wide Web at \mbox{\url{https://github.com/jfinkels/ncapproximation}}.
It is also licensed under the Creative Commons Attribution-ShareAlike License.

The author can be contacted via email at \email{jeffreyf@bu.edu}.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
