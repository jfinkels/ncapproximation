\documentclass[]{article}

% Package `amsthm` and `thmtools` must come before package `hyperref`.
\usepackage{amsthm}
\usepackage{thmtools}
% Package `hyperref` must come before package `complexity`.
\usepackage[pdftitle={Highly parallel approximations for inherently sequential problems}, pdfauthor={Jeffrey Finkelstein}]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{complexity}
\usepackage{tikz}

\declaretheorem[numberwithin=section]{theorem}
\declaretheorem[numberlike=theorem]{conjecture}
\declaretheorem[numberlike=theorem]{corollary}
\declaretheorem[numberlike=theorem]{proposition}
\declaretheorem[numberlike=theorem]{todo}
\declaretheorem[numberlike=theorem, style=definition]{definition}
\declaretheorem[numberlike=theorem, style=definition, name=Open question]{openquestion}

\newenvironment{justification}{\begin{proof}[Justification]}{\end{proof}}

\newcommand{\Er}{\leq_E^{L}}
\newcommand{\APr}{\leq_{AP}^{L}}
\newcommand{\email}[1]{\href{mailto:#1}{\nolinkurl{#1}}}
\newcommand{\lb}{\left\{}
\newcommand{\rb}{\right\}}
\newcommand{\st}{\,\middle|\,}
\newcommand{\cl}{\operatorname{cl}}

%% Fixes spacing around \left and \right operators.
%% Source: http://tex.stackexchange.com/a/2610
\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

\author{Jef{}frey Finkelstein}
\date{\today}
\title{Highly parallel approximations for inherently sequential problems}

\begin{document}

\maketitle

\section{Introduction}

In this work we study classes of optimization problems that require inherently sequential algorithms to solve exactly but permit highly parallel algorithms for approximation solutions.
\NC{} is the class of computational problems decidable by a logarithmic space uniform family of Boolean circuits of bounded fan-in, polynomial size, and polylogarithmic depth.
Such problems are considered both ``efficient'' (since $\NC \subseteq \P$) and ``highly parallel'' (since we might consider each gate in the circuit to be a processor working in parallel and the small depth of the circuit a small number of steps).
By contrast, problems which are \P-complete (under logarithmic space or even \NC{} many-one reductions) are considered ``inherently sequential''.
Furthermore, all \NP-hard and \PSPACE-hard problems are also inherently sequential, since $\P \subseteq \NP \subseteq \PSPACE$.
Just as we hope to find efficient approximation algorithms for optimization problems for which it is intractable to compute an exact solution, so too do we hope to find efficient and highly parallel approximation algorithms for optimization problems for which computing the exact solution is inherently sequential.
(However, just like hardness of approximation for \NP-hard problems, in some cases even \emph{approximating} a solution is inherently sequential!)

\section{Definitions for \texorpdfstring{\NC}{NC} approximation classes}

Throughout this work, $\Sigma=\{0, 1\}$ and inputs and outputs are encoded in binary.
The set of all finite strings is denoted $\Sigma^*$, and for each $x\in\Sigma^*$, we denote the length of $x$ by $|x|$.
We denote the set of all polynomials by \poly{} and the set of all polylogarithmic functions by \polylog.

\subsection{Optimization problems, approximators, and approximation schemes}

We adapt the clear and concise definitions of \cite{tantau07} from logarithmic space approximability to \NC{} approximability.

\begin{definition}[\cite{acgkmp99}]
  An \emph{optimization problem} is a four-tuple, $(I, S, m, t)$, where the set $I \subseteq \Sigma^*$ is called the \emph{instance set}, the set $S \subseteq I \times \Sigma^*$ is called the \emph{solution relation}, the function $m \colon S \to \mathbb{N}^+$ is called the \emph{measure function}, and $t \in \{\min, \max\}$ is called the \emph{type} of the optimization.
\end{definition}

\begin{definition}[\cite{tantau07}]
  Let $P$ be an optimization problem, so $P = (I, S, m, t)$, and let $x\in I$.
  \begin{enumerate}
  \item Let $S(x)=\lb y\in\Sigma^* \st (x, y)\in S \rb$; we call this the \emph{solutions for $x$}.
  \item Define $m^*(x)$ by
    \begin{displaymath}
      m^*(x) =
      \begin{cases}
        \min \lb m(x, y) \st y\in S(x) \rb & \text{if } t = \min \\
        \max \lb m(x, y) \st y\in S(x) \rb & \text{if } t = \max
      \end{cases}
    \end{displaymath}
    for all $x\in \Sigma^*$; we call this the \emph{optimal measure for $x$}.
    Let $m^*(x)$ be undefined if $S(x)=\emptyset$.
  \item Let $S^*(x)=\lb y\in\Sigma^* \st m(x, y) = m^*(x) \rb$; we call this the \emph{set of optimal solutions for $x$}.
  \item Let $R(x, y)=\max\left( \frac{m(x, y)}{m^*(x)}, \frac{m^*(x)}{m(x, y)} \right)$; we call this the \emph{performance ratio of the solution $y$}.
  \item Let $P_\exists = \lb x\in \Sigma^* \st S(x) \neq \emptyset \rb$; we call this the \emph{existence problem}.
  \item Let
    \begin{displaymath}
      P_{opt<}=\lb (x, z) \in P_\exists\times\mathbb{N} \st \exists y\in\Sigma^*\colon m(x, y) < z \rb
    \end{displaymath}
    and
    \begin{displaymath}
      P_{opt>}=\lb (x, z) \in P_\exists\times\mathbb{N} \st \exists y\in\Sigma^*\colon m(x, y) > z \rb;
    \end{displaymath}
    call these the \emph{budget problems}.
  \item Let $f\colon \Sigma^*\to\Sigma^*$.
    We say \emph{$f$ produces solutions for $P$} if for all $x\in P_\exists$ we have $f(x)\in S(x)$.
    We say \emph{$f$ produces optimal solutions for $P$} if for all $x\in P_\exists$ we have $f(x)\in S^*(x)$.
  \end{enumerate}
\end{definition}

The performance ratio $R(x, y)$ is a number in the interval $[1, \infty)$.
The closer $R(x, y)$ is to 1, the better the solution $y$ is for $x$, and the closer $R(x, y)$ to $\infty$, the worse the solution.

\begin{definition}
  Let $P$ be an optimization problem, let $r\colon \mathbb{N}\to\mathbb{N}$, and let $f\colon I\to \Sigma^*$.
  We say $f$ is an \emph{$r$-approximator for $P$} if it produces solutions for $P$ and $R(x, f(x)) \leq r(|x|)$ for all $x\in P_\exists$.

  If $r$ is the constant function with value $\delta$, we simply say $f$ is a \emph{$\delta$-approximator for $P$}.
\end{definition}

\begin{definition}
  Let $P$ be an optimization problem and let $f\colon I\times\mathbb{N}\to\Sigma^*$.
  We say $f$ is an \emph{approximation scheme for $P$} if for all $x\in P_\exists$ and all positive integers $k$ we have $f(x, k)\in S(x)$ and $R(x, f(x, k)) \leq 1 + \frac{1}{k}$.
\end{definition}

\subsection{Classes of optimization problems}

The study of \emph{efficient} approximations for \emph{intractable} problems begins with the following definition of \NP{} optimization problems.
We will adapt this definition to explore \emph{efficient and highly parallel} approximations for \emph{inherently sequential} problems.

\begin{definition}\label{def:npo}
  The complexity class \NPO{} is the class of all optimization problems $(I, S, m, t)$ such that the following conditions hold.
  \begin{enumerate}
  \item The instance set $I$ is decidable by a deterministic polynomial time Turing machine.
  \item The solution relation $S$ is decidable by a deterministic polynomial time Turing machine and is polynomially bounded (that is, the length of $y$ is bounded by a polynomial in the length of $x$ for all $(x, y)\in S$).
  \item The measure function $m$ is computable by a deterministic polynomial time Turing machine.
  \end{enumerate}
\end{definition}

The second condition is the most important in this definition; it is the analog of polynomial time verifiability in \NP.

\begin{definition}
  The complexity class \PO{} is the subclass of \NPO{} in which for each optimization problem $P$ there exists a function $f$ in \FP{} that produces optimal solutions for $P$.
\end{definition}

We now wish to translate these definitions to the setting of efficient and highly parallel verifiability.
In order to take advantage of results and techniques from the study of \NPO{} and \PO, we will start by considering the nondeterministic analog of \PO.
First we define the necessary circuit classes, then we define the corresponding classes of approximation problems.

\begin{definition}
  \mbox{}
  \begin{enumerate}
  \item \NC{} is the class of decision problems decidable by a logarithmic space uniform family of Boolean circuits with polynomial size, polylogarithmic depth, and fan-in two.
  \item \FNC{} is the class of functions $f$ computable by an \NC{} circuit in which the output of the circuit is (the binary encoding of) $f(x)$.
  \item $\NNC(f(n))$ is the class of languages computable by a logarithmic space uniform \NC{} circuit family augmented with $O(f(n))$ nondeterministic gates for each input length $n$ \cite{wolf94}.
    A nondeterministic gate takes no inputs and yields a single (nondeterministic) output bit.

    If $\mathcal{F}$ is a class of functions, then $\NNC(\mathcal{F})=\bigcup_{f\in\mathcal{F}}{\NNC(f(n))}$.
  \end{enumerate}
\end{definition}

\NNCpoly, also known as $\GC(\poly, \NC)$ \cite{cc97} and $\beta\P$ \cite{kf80}, is an unusual class which may warrant some further explanation.
\NC{} has the same relationship to \NNCpoly{} as \P{} does to \NP{} (thus an equivalent definition of \NNCpoly{} is one in which each language has an efficient and highly parallel verification procedure; as in the definition of \NPO{} in \autoref{def:npo}, it is this formulation which we use when defining \NNCO{} in \autoref{def:nnco}).
Wolf \cite{wolf94} notes that $\NNC(\log n)=\NC$ and $\NNC(\poly)=\NP$, and suggests that \NNC(\polylog) may be an interesting intermediary class, possibly incomparable with \P.
Cai and Chen \cite{cc97} prove that for each natural number $k$ and $i$, there is a complete problem for $\NNC^k\left(\log^i n\right)$ under logarithmic space many-one reductions.

\begin{definition}\label{def:nnco}
  The complexity class \NNCOpoly{} is the class of all optimization problems $(I, S, m, t)$ such that the following conditions hold.
  \begin{enumerate}
  \item The instance set $I$ is decidable by an \NC{} circuit family.
  \item The solution relation $S$ is decidable by an \NC{} circuit family and is polynomially bounded (that is, the length of $y$ is bounded by a polynomial in the length of $x$ for all $(x, y)\in S$).
  \item The measure function $m$ is computable by an \FNC{} circuit family.
  \end{enumerate}
  For the sake of brevity, we write \NNCO{} instead of \NNCOpoly{}.
\end{definition}

We can now proceed to define classes of approximable optimization problems contained in \NNCO.

\begin{definition}\label{def:ncx}
  Suppose $P$ be an optimization problem in \NNCO.
  \begin{enumerate}
  \item $P \in \ApxNCO$ if there is an $r$-approximator in \FNC{} for $P$, where $r(n) \in O(1)$ for all $n \in \mathbb{N}$.
  \item $P \in \NCAS$ if there is an approximation scheme $f$ for $P$ such that $f_k \in \FNC$ for each $k \in \mathbb{N}$, where $f_k(x) = f(x, k)$ for all $x \in \Sigma^*$.
  \item $P \in \FNCAS$ if there is an approximation scheme $f$ for $P$ such that $f \in \FNC$ in the sense that the size of the circuit is polynomial in both $|x|$ and $k$ and the depth of the circuit is polylogarithmic in both $|x|$ and $k$.
  \item $P \in \NCO$ if there is a function $f$ in \FNC{} that produces optimal solutions for $P$.
  \end{enumerate}
\end{definition}

For the \NC{} approximation classes defined above, it is crucial that the solution relation is verifiable in \NC{}.
In all previous works (for example, \cite{dsst97, sx95}), the implicit definition of, say, \NCX{}, which corresponds to our class \ApxNCO, requires only that the solution relation is verifiable \emph{in polynomial time}.
We believe we are the first to make this important distinction; the maximization problem defined in the proof of, for example, \autoref{prop:nnconpo} below shows that some solution relations are harder to verify than others.

Each of the classes in \autoref{def:ncx} includes the one defined below it.
This chain of inclusions provides a hierarchy that classifies approximability of problems in \NNCO{}, and hence in \NPO.
However, our intention is to determine the approximability of optimization problems corresponding to \P-complete decision problems, not those corresponding to \NP-complete decision problems.
Therefore we consider the classes $\PO \cap \NNCO$, $\PO \cap \ApxNCO$, etc. in order to more accurately capture the notion of highly parallel approximability of inherently sequential problems.
The instance set, solution relation, and measure function of optimization problems in these classes are computable in \NC{}, and furthermore, there is a polynomial time algorithm that produces optimal solutions.

\autoref{fig:inclusions} shows the inclusions among some of the complexity classes defined in this section.
\begin{figure}
  \caption{%
    Inclusions among some classes of approximable optimization problems.
    The complexity class at the tail of each arrow is a subset of the complexity class at the head of the arrow.
    \label{fig:inclusions}}
  \begin{center}
    \begin{tikzpicture}

      \draw (0, 7) node(NPO) {\NPO};

      \draw (-2, 5) node(NNCO) {\NNCO};
      \draw (-2, 4) node(ApxNCO) {\ApxNCO};
      \draw (-2, 3) node(NCAS) {\NCAS};

      \draw (1, 6) node(ApxPO) {\ApxPO};
      \draw (1, 5) node(PTAS) {\PTAS};
      \draw (1, 4) node(PO) {\PO};
      \draw (1, 3) node(POp) {$\PO \cap \NNCO$};
      \draw (1, 2) node(ApxNCOp) {$\ApxNCO \cap \NNCO$};
      \draw (1, 1) node(NCASp) {$\NCAS \cap \NNCO$};

      \draw (1, 0) node(NCO) {\NCO};

      \path[->]
      (NCO) edge (NCASp)
      (NCASp) edge (ApxNCOp)
      (ApxNCOp) edge (POp)
      (POp) edge (PO)
      (PO) edge (PTAS)
      (PTAS) edge (ApxPO)
      (ApxPO) edge (NPO)

      (NCAS) edge (ApxNCO)
      (ApxNCO) edge (NNCO)

      (NCASp) edge (NCAS)
      (ApxNCOp) edge (ApxNCO)
      (POp) edge (NNCO)

      (NCAS) edge (PTAS)
      (ApxNCO) edge (ApxPO)
      (NNCO) edge (NPO);
    \end{tikzpicture}
  \end{center}
\end{figure}

\subsection{Reductions among approximation problems}

There are many reductions for approximation problems; nine of them are defined in a survey paper by Crescenzi \cite{crescenzi97}, and there are more defined elsewhere.
We will use a logarithmic space-bounded version of the ``?? reduction''.%, considered by approximation experts to be the a reasonable reduction to use when constructing complete problems \cite[Section~2]{crescenzi97} \cite[Section~8.6]{acgkmp99}.

%% \begin{definition}{\cite[Definition~9]{ckst95}}
%%   Let $P$ and $Q$ be optimization problems in \NNCO, with $P=(I_P, S_P, m_p, t_P)$ and $Q=(I_Q, S_Q, m_Q, t_Q)$.
%%   We say \emph{$P$ AP reduces to $Q$} and write $P\APr Q$ if there exist functions $f\colon\Sigma^*\to\Sigma^*$ and $g\colon\Sigma^*\to\Sigma^*$, and there exists a constant $\alpha\in\mathbb{R}$ such that:
%%   \begin{enumerate}
%%   \item For all $x\in I_P$ and all $r > 1$, we have $f(x, r)\in I_Q$.
%%   \item For all $x\in I_P$, all $r > 1$, and all $y\in S_Q(f(x, r))$, we have $g(x, y, r)\in S_P(x)$.
%%   \item $f$ and $g$ are computable in logarithmic space for any fixed $r$.
%%   \item For all $x\in I_P$, all $r > 1$, and all $y\in S_Q(f(x, r))$, we have $R_Q(f(x, r), y) \leq r \implies R_P(x, g(x, y, r)) \leq 1 + \alpha(r - 1)$.
%%   \end{enumerate}
%% \end{definition}

%% We bound the AP reduction to logarithmic space instead of allowing it be a \FNC{} circuit family because 1.~the former notion of reduction is more restrictive and hence implies the latter, 2.~existing results on approximability and approximation classes use the former, and 3.~it eases analysis in some proofs.

For a class $\mathcal{C}$ of optimization problems, we say a problem $Q$ is \emph{$\mathcal{C}$-hard} if for all problems $P$ in $\mathcal{C}$ there is a ??? reduction from $P$ to $Q$.
If furthermore $Q$ is in $\mathcal{C}$ we say $Q$ is \emph{$\mathcal{C}$-complete}.

\section{Relations among classes of optimization problems}

Since $\NC \subseteq \P$, the inclusion $\NNCO \subseteq \NPO$ follows immediately from the definitions.
However, these definitions do not imply that the converse is true, since membership in \P{} does not imply membership in \NC.
In fact, there is evidence that $\NNCO \neq \NPO$.
\begin{proposition}[Thanks to Sam for this one.]\label{prop:nnconpo}
  $\NNCO = \NPO$ if and only if $\P = \NC$.
\end{proposition}
\begin{proof}
  We know $\NNCO \subseteq \NPO$ unconditionally from the preceding paragraph.
  If we suppose $\P = \NC$, then it follows immediately from the definitions that any optimization problem in $\NPO$ is also in $\NNCO$.

  Now suppose $\NNCO = \NPO$.
  If $C$ is a Boolean circuit with a single output, let $C(x)$ denote the output of $C$ (as a bit).
  Consider the optimization problem $Q$ defined by $Q = (I, S, m, t)$, where
  \begin{align*}
    I & = \{ C \, | \, C \text{ is a Boolean circuit} \}, \\
    S & = \{ (C, x) \, | \, C(x) = 1 \}, \\
    m(C, x) & = C(x), \text{ and} \\
    t & = \max.
  \end{align*}
  (Observe that $Q$ is an optimization form of the \textsc{Circuit Satisfiability} problem, where we have $m(C, x) \in \{0, 1\}$ for all circuits $C$ and all strings $x$.)
  $I \in \P$ because a correctly formatted Boolean circuit is a directed acyclic graph, and a depth first search (computable in deterministic polynomial time) can reveal if a given directed graph has a cycle.
  $S \in \P$ because it is exactly the \textsc{Circuit Value} problem, which is known to be in \P{} (in fact, it is \P-complete \cite[Problem~A.1.1]{ghr95}).
  $m \in \P$ because it is simply the function form of the \textsc{Circuit Value} problem.
  Therefore, $Q \in \NPO$.

  Since $\NNCO = \NPO$ by hypothesis, $Q \in \NNCO$.
  Hence, $S \in \NC$.
  The existence of an \NC{} algorithm for a \P-complete problem implies $\P = \NC$.
  Therefore, $\NNCO = \NPO$ if and only if $\P = \NC$.
\end{proof}

The same result holds in the intersection with $\PO$.

\begin{proposition}\label{prop:poppo}
  $\PO \cap \NNCO = \PO$ if and only if $\P = \NC$.
\end{proposition}
\begin{proof}
  As in \autoref{prop:nnconpo}, we consider a language in \PO{} that is unlikely to have a solution relation decidable in \NC{}.
  Consider the optimization problem $Q$ defined by $Q = (I, S, m, t)$, where
  \begin{align*}
    I & = \{ (C, x) \, | \, C \text{ is a Boolean circuit and } x \text{ is an input to } C \}, \\
    S & = \{ ((C, x), b) \, | \, b \in \{0, 1\} \text{ and } C(x) = b \}, \\
    m((C, x), b) & = b, \text{ and} \\
    t & = \max.
  \end{align*}
  Here we have an optimization version of the \textsc{Circuit Value} problem.
  The solution relation $S$ is in \P, and again it is \P-complete.
  If $\PO = \PO \cap \NNCO$, then there is an \NC{} algorithm for this \P-complete problem, and hence $\P = \NC$.
\end{proof}

The classes of approximable optimization problems are also likely distinct.

\begin{theorem}\label{thm:hierarchy}
  \mbox{}
  \begin{enumerate}
  \item If $\NC \neq \NP$ then $\NCO \subsetneq \NCAS \subsetneq \ApxNCO \subsetneq \NNCO$.
  \item If $\NC \neq \P$ then $\NCO \subsetneq \PO \cap \NCAS \subsetneq \PO \cap \ApxNCO \subsetneq \PO \cap \NNCO \subsetneq \PO$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  \begin{todo}
    Fill me in.
  \end{todo}
\end{proof}

What is the relationship between the class $\PO \cap \NNCO$ and the \NC{} approximation classes?
We adapt \cite[Theorem~8.2.9]{dsst97} to our setting.
\begin{todo}
  This is a special case of \autoref{thm:hierarchy}.
\end{todo}

\begin{proposition}\label{prop:popapxnco}
  If $\PO \cap \NNCO \subseteq \ApxNCO$ then $\P = \NC$.
\end{proposition}
\begin{proof}
  Suppose $\PO \cap \NNCO \subseteq \ApxNCO$.
  Consider the maximization problem \textsc{Linear Programming}, defined as follows.
  \begin{align*}
    I & = \left\{(A, \mathbf{b}, \mathbf{c}) \, \middle| \, A \in \mathbb{Z}^{m \times n}, \mathbf{b} \in \mathbb{Z}^m, \mathbf{c} \in \mathbb{Z}^n \right\} \\
    S & = \left\{((A, \mathbf{b}, \mathbf{c}), \mathbf{x}) \, \middle| \, \mathbf{x} \in \mathbb{Q}^n \text{ and } A\mathbf{x} \leq \mathbf{b}\right\} \\
    m((A, \mathbf{b}, \mathbf{c}), \mathbf{x}) & = \mathbf{c}^\intercal\mathbf{x} \\
    t & = \max
  \end{align*}
  This problem is in \PO{} by the ellipsoid algorithm \cite{khachian79}.
  The problem is also in \NNCO{}, since a solution can be verified in \NC{} (via matrix multiplication and comparison of rational numbers) and the measure can be computed in \NC{} (again by matrix multiplication).
  Therefore it is in the class $\PO \cap \NNCO$.

  By hypothesis, the problem is also in \ApxNCO{}, so the problem has an $r$-approximator computable in \NC{} for some $r > 1$.
  By \cite[Theorem~8.2.7]{dsst97}, the decision problem of $r$-approximating \textsc{Linear Programming} is \P-complete for all $r > 1$.
  Thus we have an \NC{} algorithm for a \P-complete problem, and hence $\NC = \P$.
\end{proof}

As an analog to \autoref{prop:nnconpo}, we have the following corollary of the previous proposition.

\begin{corollary}[{\cite[Theorem~8.2.9]{dsst97}}]
  $\ApxNCO = \ApxPO$ if and only if $\P = \NC$.
\end{corollary}

\section{Completeness in classes of approximable optimization problems}

First, we show that \textsc{Maximum Variable-Weighted Satisfiability} is complete for the class of maximization problems in \NNCO.

\begin{definition}[\textsc{Maximum Variable-Weighted Satisfiability}]
  \mbox{} \\
  \begin{tabular}{r p{9.5cm}}
    \textbf{Instance:} & Boolean formula $\phi$ on variables $x_1, \dotsc, x_n$, weights in $\mathbb{Q}^+$ for each variable $w_1, \dotsc, w_n$. \\
    \textbf{Solution:} & assignment $\alpha$ to the variables that satisfies $\phi$. \\
    \textbf{Measure:} & $\Sigma_{i = 1}^n \alpha(x_i) w_i$. \\
    \textbf{Type:} & maximization.
  \end{tabular}
\end{definition}

\begin{theorem}
  \textsc{Maximum Variable-Weighted Satisfiability} is complete for the class of maximization problems in \NNCO{} under logarithmic space ??? reductions.
\end{theorem}
\begin{proof}
  First, this problem is in \NNCO.
  The measure function is computable in \FNC{} because the basic arithmetic operations and summation are both computable in \FNC{}.
  The solution set is decidable in \NC{} because Boolean formula evaluation is computable in \NC{} \cite{buss87}.

  The proof that shows the problem is hard for \NNCO{} is similar to the one that shows \textsc{Maximum Variable-Weighted Satisfiability} is hard for \NPO{} \cite[Theorem~3.1]{om87} \cite[Theorem~8.3]{acgkmp99}.
  The only difference is that the underlying machines are \NC{} and \NNC{} machines instead of \P{} and \NP{} machines.
\end{proof}

Next, we show a maximization problem that is complete for the class of maximization problems in \PO.

\begin{definition}[\textsc{Double Circuit Value}]
  \mbox{} \\
  \begin{tabular}{r l}
    \textbf{Instance:} & two Boolean circuits $C_1$ and $C_2$, binary string $x$. \\
    \textbf{Solution:} & binary string $y$ such that $C_1(x) = y$. \\
    \textbf{Measure:} & $C_2(x, y)$. \\
    \textbf{Type:} & maximization.
  \end{tabular}
\end{definition}

This problem is constructed so that the circuit $C_1$ can simulate an algorithm that produces an optimal solution for an optimization problem and the circuit $C_2$ can simulate an algorithm that outputs the measure of a solution for that problem.

\begin{theorem}
  \textsc{Double Circuit Value} is complete under ??? reductions for the class of maximization problems in \PO.
\end{theorem}
\begin{proof}
  Since \textsc{Circuit Value} is in \P, both the solution and the measure function are computable in polynomial time.
  Therefore \textsc{Double Circuit Value} is in \PO.
  Our goal is now to exhibit a ??? reduction from any language in \PO{} to \textsc{Double Circuit Value}.
  For the sake of brevity, suppose \textsc{Double Circuit Value} is defined by $(I_C, S_C, m_C, \max)$.

  Let $P$ be a maximization problem in \PO, where $P = (I_P, S_P, m_P, \max)$.
  Let $x$ be an element of $I_P$.
  Suppose $E$ is the deterministic polynomial time Turing machine that produces optimal solutions for $P$.
  Define $f$ by $f(x) = (C_E, C_m, x)$ for all $x \in I_P$, where $C_E$ is the Boolean circuit of polynomial size that simulates the action of $E$ on input $x$ and $C_m$ is the circuit that simulates $m_P$ on inputs $x$ and $E(x)$.
  These circuits exist and are computable from $x$ in logarithmic space \cite{ladner75}.
  Define $g$ by $g(x, y) = y$ for all strings $x$ and all $y$ in $S_C(f(x))$.

  %% We know that $m^*_C(f(x)) = m^*_C((C_E, C_m, x))$, which equals the maximum over any string $y$ such that $C_E(x) = y$ of the value of the circuit $C_m$ on input $x$ and $y$.
  %% The fact that $E$ produces optimal solutions for $P$ and the correctness of the mappings $(E, x) \mapsto C_E$ and $(m_P, x, y) \mapsto C_m$ imply that $m^*((C_E, C_m, x)) = m^*_P(x)$.
  For any $x \in I_P$ and any $y \in S_C(f(x))$, we have
  \begin{equation*}
    m_P(x, g(x, y)) = m_P(x, y) = C_m(x, y) = m_C((C_E, C_m, x), y) = m_C(f(x), y).
  \end{equation*}
  Since these measures are equal for all instances $x$ and solutions $y$, we have shown a logarithmic space ??? reduction from $P$ to \textsc{Double Circuit Value}.
\end{proof}

\begin{todo}
  Show that \textsc{Double Circuit Value} is complete for minimization problems as well, then change the theorems and discussion in section.
\end{todo}

Since the solution relation in \textsc{Double Circuit Value} requires evaluating a Boolean circuit, this problem is not in \NNCO{}, unless $\P = \NC$.
However, there are problems that are both complete for \PO{} and in \NNCO.

\begin{theorem}
  \textsc{Linear Programming} is complete for the class of all maximization problems in \PO{} under logarithmic space ??? reductions.
\end{theorem}
\begin{proof}
  \textsc{Linear Programming} is in \PO{} by the ellipsoid algorithm.
  We reduce \textsc{Double Circuit Value} to \textsc{Linear Programming}.
  The reduction is essentially the same as the reduction from \textsc{Circuit Value} to \textsc{Linear Programming} given (implicitly) in the hint beneath \cite[Problem~A.4.1]{ghr95}.
  We repeat it here for the sake of completeness.

  Define the instance transducer $f$ as follows.
  Suppose $(C_1, C_2, x)$ is an instance of \textsc{Double Circuit Value}, and let $x = x_1 \dotsb x_n$.
  For each of the circuits $C_1$ and $C_2$, the transducer $f$ adds the following inequalities to the linear program.
  \begin{enumerate}
  \item For each bit of $x$, represent a 1 bit at index $i$ by $x_i = 1$ and a 0 bit by $x_i = 0$.
  \item Represent a \textsc{not} gate, $g = \lnot h$, by the equation $g = 1 - h$ and the inequality $0 \leq g \leq 1$.
  \item Represent an \textsc{and} gate, $g = h_1 \land h_2$, by the inequalities $g \leq h_1$, $g \leq h_2$, $h_1 + h_2 - 1 \leq g$, and $0 \leq g \leq 1$.
  \item Represent an \textsc{or} gate, $g = h_1 \lor h_2$, by the inequalities $h_1 \leq g$, $h_2 \leq g$, $g \leq h_1 + h_2$, and $0 \leq g \leq 1$.
  \end{enumerate}
  Suppose $y_1, \dotsc, y_s$ are the variables corresponding to the output gates of $C_1$, and suppose $\mu_t, \dotsc, \mu_1$ are the variables corresponding to the output gates of $C_2$, numbered from least significant bit to most significant bit (that is, right-to-left).
  The components of the object function $\mathbf{c}$ are assigned to be $2^i$ where the component corresponds to the variable $\mu_i$ and 0 everywhere else.
  The function $f$ is computable in logarithmic space because the transformation can proceed gatewise, requiring only a logarithmic number of bits to record the index of the current gate.
  Suppose $\mathbf{x}$ is a solution to $f((C_1, C_2, x))$, that is, an assignment to the variables described above that satisfies all the inequalities.
  Define the solution transducer $g$ by $g((C_1, C_2, x), \mathbf{x}) = y$, where $y = y_1 \dotsb y_s$.
  This is also computable in logarithmic space by finding the index, in binary, of the necessary gates $y_1, \dotsc, y_s$.

  By structural induction on the gates of the circuits we see that a gate has value 1 on input $x$ if and only if the solution vector $\mathbf{x}$ has a value 1 in the corresponding component, and $\mathbf{x}$ must be a vector over $\{0, 1\}$.
  Since the linear program correctly simulates the circuits, we see that
  \begin{align*}
    m_A((C_1, C_2, x), g((C_1, C_2, x), \mathbf{x})) & = m_A((C_1, C_2, x), y) \\
    & = C_2(x, y) \\
    & = \mu_t \dotsb \mu_1 \\
    & = \Sigma^t_{i = 1} 2^i \mu_i \\
    & = m_B(f((C_1, C_2, x)), \mathbf{x}),
  \end{align*}
  where $m_A$ is the measure function for \textsc{Double Circuit Value} and $m_B$ is the measure function for \textsc{Linear Programming}.
  Since these measures are equal, we have shown a logarithmic space ??? reduction from \textsc{Double Circuit Value} to \textsc{Linear Programming}.
  Since the former is complete for the class of maximization problems in \PO, so is \textsc{Linear Programming}.
\end{proof}

As discussed in \autoref{prop:popapxnco}, \textsc{Linear Programming} is in $\PO \cap \NNCO$.
This is interesting because, for the same reasons as in the proof of \autoref{prop:poppo}, \textsc{Double Circuit Value} is not in $\PO \cap \NNCO$ unless $\P = \NC$.
This yields the following corollary.

\begin{corollary}\label{cor:lpishard}
  \mbox{}
  \begin{enumerate}
  \item \textsc{Linear Programming} is complete for the class of all maximization problems in $\PO \cap \NNCO$ under logarithmic space ??? reductions.
  \item \textsc{Double Circuit Value} is \emph{not} complete for the class of all maximization problems in $\PO \cap \NNCO$ under logarithmic space ??? reductions unless $\P = \NC$.
  \end{enumerate}
\end{corollary}

This corollary suggests an explanation for both \autoref{prop:popapxnco} and the following well-known theorem.

\begin{theorem}[TODO FILL ME IN WITH REFERENCE]
  The problem of $r$-approximating \textsc{Linear Programming} for any $r \geq 1$ is \P-complete.
\end{theorem}

Since a constant factor \NC{} approximation algorithm for a problem complete for $\PO \cap \NNCO$ induces a constant factor \NC{} approximation algorithm for all maximization problems in $\PO \cap \NNCO$, there can be no such algorithm for \textsc{Linear Programming}, unless $\P = \NC$.
In fact, we can say something much stronger: \autoref{cor:lpishard}, \autoref{thm:hierarchy}, and the fact that ??? reductions compose imply that any highly parallel approximation for \textsc{Linear Programming} necessitates $\P = \NC$.

\begin{corollary}
  For any function $r$, \textsc{Linear Programming} does not have an $r$-approximation algorithm in \NC{} unless $\P = \NC$.
\end{corollary}

\begin{todo}
  Is this true? Check out the meaning of \NPO-completeness and translate to $\NNCO \cap \PO$-completeness.
\end{todo}

%% \begin{theorem}[{\cite[{Theorem~4 and Theorem~5}]{am84}}]
%%   All of the following problems are in \NC{} for all $\epsilon > 2$ but \P-complete for all $\epsilon \in [1, 2)$.
%%   \begin{itemize}
%%   \item \textsc{$\epsilon$-High Degree Subgraph} \cite[Theorem~4 and Theorem~5]{am84}
%%   \item \textsc{$\epsilon$-High Connectivity Subgraph} \cite[Theorem~6]{ss89}
%%   \end{itemize}
%% \end{theorem}

%% \begin{conjecture}
%%   All of the following problems are complete for \ApxNCOp{} under $\APr$ reductions.
%%   \begin{itemize}
%%   \item \textsc{Maximum High Degree Subgraph}
%%   \item \textsc{Maximum High Connectivity Subgraph}
%%   \end{itemize}
%% \end{conjecture}

\begin{todo}
  Produce a \P-complete problem which is complete for \ApxNCO{} (and then for each of the other approximation classes) by considering a restriction of an \NP-complete problem and following the same proof.
\end{todo}

\begin{todo}
  Produce a \P-complete problem which is complete for \ApxNCO{} (and then for each of the other approximation classes) by considering a relaxation of an \NL-complete problem and following a similar proof (this will be harder because a logarithmic space-bounded Turing machine can be described by a small directed graph representing its configurations and transitions).
\end{todo}

\begin{todo}
  Suppose $Q$ is an optimization problem, so $Q=(I, S, m, t)$, and $\hat{Q}$ is the budget problem for $Q$ which corresponds to $t$.
  Is it true that if $Q$ is \PO-complete then $\hat{Q}$ is \P-complete?
  Is the converse true?
\end{todo}

\begin{todo}[Thanks to Rita for this one]
  Do there exist, for example, \ApxPO-complete optimization problems which are based on both \NP-complete problems and \PSPACE-complete problems?
  \STP-complete problems?
\end{todo}

\textsc{Positive Linear Programming}, the restriction of \textsc{Linear Programming} to inputs in which all entries of $A$, $\mathbf{b}$, and $\mathbf{c}$ are non-negative, admits a fully \NC{} approximation scheme \cite{ln93}, even though the corresponding budget problem remains \P-complete \cite{tx98}.
In fact, we can reduce a restricted form of \textsc{Linear Programming}, $k$-normal form, to \textsc{Positive Linear Programming}. \cite{trevisan00}
\begin{openquestion}
  Is there a restriction of \textsc{Linear Programming} weaker than the restriction of non-negativity that permits a constant-factor \NC{} approximation?
\end{openquestion}

\section{Syntactic characterization of \texorpdfstring{\ApxNCO}{ApxNCO}}

In this section, $\cl(\mathcal{C}, \leq)$ denotes the closure of the complexity class $\mathcal{C}$ under $\leq$ reductions.

In \cite{py91}, the authors introduce a wealth of problems which are complete under ``L reductions'' for \MaxSNP, the class of maximization problems in strict \NP{} (\SNP), which is a syntactic characterization of \NP.
Further work showed that $\cl\left(\MaxSNP, \leq^P_E\right) = \ApxPO_{pb}$ \cite[Theorem~1]{kmsv99}, and since $\cl\left(\ApxPO_{pb}, \leq^P_{PTAS}\right) = \ApxPO$ \cite{ct00}, we conclude $\cl\left(\MaxSNP, \leq^P_{PTAS}\right) = \ApxPO$ \cite{kmsv99}.
We also have $\cl\left(\MaxSNP, \leq^P_E\right) = \cl\left(\MaxNP, \leq^P_E\right)$ \cite[Theorem~2]{kmsv99} and \textsc{Maximum Satisfiability} is complete for \MaxNP{} under $\leq^P_E$ reductions.
Can these results translate to \NC{} in any way?

According to \cite[Theorem~9.1.3]{dsst97}, $\MaxSNP \subseteq \NCX$.
They also ask the provide the following open question: is there a (possibly weaker) reduction $\leq^?_?$ such that $\cl\left(\MaxSNP, \leq^?_?\right) = \NCX$?
It may help to know that if $P$ is complete for $\MaxSNP$ under $\leq^L_L$ reductions, then $P$ exhibits threshold behavior for constant factor \NC{} approximation algorithms \cite[Theorem~9]{sx95} (see also \cite[Theorem~9.2.3]{dsst97}).

\begin{todo}
  We know that $\FO[\polylog] = \NC$ \cite[Theorem~5.2]{immerman99}; can we use this to construct a syntactic definition of $\ApxNCO$?
  Can we more easily construct a complete problem using this characterization?
\end{todo}

\section{Translating hardness of approximation results to \texorpdfstring{\NC}{NC}}

The celebrated \PCP{} theorem gives a precise characterization of \NP{} in terms of probabilistic verifiers with bounded randomness and queries, specifically that $\NP = \PCP(\log n, 1)$ \cite{pcp}.
This characterization allows many hardness of approximation results for optimization problems in \NPO{} corresponding to \NP-complete decision problems.

\begin{todo}
  Can the same be done for \NNC?
  Is there a \PCP{} characterization for \NNC?
\end{todo}

We have some initial results in this line of research \cite{finkelstein13}.

\section{About this work}

Copyright 2012, 2013 Jef{}frey Finkelstein.

This work is licensed under the Creative Commons Attribution-ShareAlike License 3.0.
Visit \mbox{\url{https://creativecommons.org/licenses/by-sa/3.0/}} to view a copy of this license.

The \LaTeX{} markup which generated this document is available on the World Wide Web at \mbox{\url{https://github.com/jfinkels/ncapproximation}}.
It is also licensed under the Creative Commons Attribution-ShareAlike License.

The author can be contacted via email at \email{jeffreyf@bu.edu}.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
