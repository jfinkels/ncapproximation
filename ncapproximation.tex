\documentclass[]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{complexity}
\usepackage[pdftitle={Definitions and reductions for NC approximation problems}, pdfauthor={Jeffrey Finkelstein}]{hyperref}

\newtheorem{conjecture}{Conjecture}
\theoremstyle{definition} \newtheorem{definition}{Definition}
\theoremstyle{definition} \newtheorem{openquestion}{Open question}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{todo}{TODO}

\mathchardef\mhyphen="2D

\newenvironment{justification}{\begin{proof}[Justification]}{\end{proof}}

\newcommand{\definitionautorefname}{Definition}

\newcommand{\Er}{\leq_E^{L}}
\newcommand{\APr}{\leq_{AP}^{L}}
\newcommand{\email}[1]{\href{mailto:#1}{\nolinkurl{#1}}}
\newcommand{\lb}{\left\{}
\newcommand{\pb}{\textsf{pb}}
\newcommand{\rb}{\right\}}
\newcommand{\st}{\,\middle|\,}

\newcommand{\apxncoproblems}{%
  \textsc{Induced Subgraph of High Weight for Linear Extremal Properties} \cite{dsst97},
  \textsc{Maximum Acyclic Subgraph} \cite[Section~7.4]{dsst97},
  \textsc{Minimum $k$-Center} \cite[Section~7.4]{dsst97},
  \textsc{$k$-Switching Network} \cite[Section~7.4]{dsst97},
  \textsc{Maximum Bounded Weighted Satisfiability} \cite[Theorem~4]{sx95}}
\newcommand{\expapxncoproblems}{}
\newcommand{\frncasproblems}{%
  \textsc{Maximum Flow} \cite[Theorem~4.5.2]{dsst97},
  \textsc{Maximum Weight Perfect Matching} \cite[Theorem~4.5.2]{dsst97},
  \textsc{Maximum Weight Matching} \cite[Theorem~4.5.2]{dsst97}}
\newcommand{\fncasproblems}{%
  \textsc{Subset Sum} \cite[Theorem~4.1.4]{dsst97},
  \textsc{Maximum Satisfiability} \cite[Theorem~8]{trevisan98},
  \textsc{Minimum Weight Vertex Cover} \cite[Theorem~5.3.6]{dsst97},
  \textsc{0-1 Knapsack} \cite[Theorem~2]{mayr88},
  \textsc{Bin Packing} \cite[Theorem~3]{mayr88}}
\newcommand{\logapxncoproblems}{}
\newcommand{\ncasproblems}{%
  \textsc{Positive Linear Programming} \cite[Theorem~5.1.11]{dsst97},
  \textsc{Maximum $k$-Constraint Satisfaction Problem} \cite[Corollary~13]{trevisan98},
  \textsc{Maximum Matching} \cite[Theorem~5.2.1]{dsst97},
  \textsc{Maximum Flow} \cite[Theorem~5.2.2]{dsst97},
  \textsc{Maximum Weight Matching} \cite[Theorem~5.2.2]{dsst97},
  \textsc{Maximum Independent Set for Planar Graphs} \cite[Theorem 6.4.1]{dsst97}}
\newcommand{\ncoproblems}{}
\newcommand{\nncoproblems}{}
\newcommand{\polyapxncoproblems}{}
\newcommand{\rncasproblems}{%
  \textsc{Minimum Metric Traveling Salesperson} \cite[Theorem~7.1.1]{dsst97}}

\author{Jef{}frey Finkelstein}
\date{\today}
\title{Definitions and reductions for \texorpdfstring{\NC}{NC} approximation problems}

\begin{document}

\maketitle

\section{Introduction}

\NC{} is the class of computational problems decidable by a Boolean circuit of polynomial size and polylogarithmic depth.
Such problems are considered both ``efficient'' (since $\NC\subseteq\P$) and ``highly parallel'' (since we might consider each gate in the circuit to be a processor working in parallel and the low depth of the circuit a small number of steps).
By contrast, problems which are \P-complete (under \NC{} reductions) are considered ``inherently sequential''.
Furthermore, all \NP-hard and \PSPACE-hard problems are also inherently sequential, since $\P\subseteq\NP\subseteq\PSPACE$.
Just as we hope to find efficient approximation algorithms for optimization problems for which it is intractable to compute an exact solution, so too do we hope to find efficient and highly parallel approximation algorithms for optimization problems for which computing the exact solution is inherently sequential.
(However, just like hardness of approximation for \NP-hard problems, in some cases even \emph{approximating} a solution within a constant factor is inherently sequential!)

\section{Definitions of \texorpdfstring{\NC}{NC} approximation classes}

Throughout this work, $\Sigma=\{0, 1\}$ and inputs and outputs are encoded in binary.
We denote the set of all polynomials by \poly{} and the set of all polylogarithmic functions by \polylog.

We adapt the clear and concise definitions of \cite{tantau07} from logarithmic space approximability to \NC{} approximability.

\begin{definition}[\cite{acgkmp99}]
  An \emph{optimization problem} is a four-tuple, $(I, S, m, t)$, where $I\subseteq \Sigma^*$ and is called the \emph{instance set}, $S\subseteq I\times \Sigma^*$ and is called the \emph{solution relation}, $m\colon S\to \mathbb{N}^+$ and is called the \emph{measure function}, and $t\in\{\min, \max\}$ and is called the type (of the optimization).
\end{definition}

\begin{definition}[\cite{tantau07}]
  Let $P$ be an optimization problem, so $P=(I, S, m, t)$, and let $x\in I$.
  \begin{enumerate}
  \item Let $S(x)=\lb y\in\Sigma^* \st (x, y)\in S \rb$; call this the \emph{solutions for $x$}.
  \item Define $m^*(x)$ by
    \begin{displaymath}
      m^*(x) =
      \begin{cases}
        \min \lb m(x, y) \st y\in S(x) \rb & \text{if } t = \min \\
        \max \lb m(x, y) \st y\in S(x) \rb & \text{if } t = \max
      \end{cases}
    \end{displaymath}
    for all $x\in \Sigma^*$; call this the \emph{optimal measure for $x$}.
    Let $m^*(x)$ be undefined if $S(x)=\emptyset$.
  \item Let $S^*(x)=\lb y\in\Sigma^* \st m(x, y) = m^*(x) \rb$; call this the \emph{set of optimal solutions for $x$}.
  \item Let $R(x, y)=\max\left( \frac{m(x, y)}{m^*(x)}, \frac{m^*(x)}{m(x, y)} \right)$; call this the \emph{performance ratio of the solution $y$}.
  \item Let $P_\exists = \lb x\in \Sigma^* \st S(x) \neq \emptyset \rb$; call this the \emph{existence problem}.
  \item Let
    \begin{displaymath}
      P_{opt<}=\lb (x, z) \in P_\exists\times\mathbb{N} \st \exists y\in\Sigma^*\colon m(x, y) < z \rb
    \end{displaymath}
    and
    \begin{displaymath}
      P_{opt>}=\lb (x, z) \in P_\exists\times\mathbb{N} \st \exists y\in\Sigma^*\colon m(x, y) > z \rb;
    \end{displaymath}
    call these the \emph{budget problems}.
  \item Let $f\colon \Sigma^*\to\Sigma^*$.
    We say \emph{$f$ produces solutions for $P$} if for all $x\in P_\exists$ we have $f(x)\in S(x)$.
    We say \emph{$f$ produces optimal solutions for $P$} if for all $x\in P_\exists$ we have $f(x)\in S^*(x)$.
  \end{enumerate}
\end{definition}

The study of \emph{efficient} approximations for \emph{intractable} problems begins with the following definition of \NP{} optimization problems.
We will adapt this definition to explore \emph{efficient and highly parallel} approximations for \emph{inherently sequential} problems.

\begin{definition}\label{def:npo}
  The complexity class \NPO{} is the class of all optimization problems $(I, S, m, t)$ such that the following conditions hold.
  \begin{enumerate}
  \item The instance set $I$ is decidable by a deterministic polynomial time Turing machine.
  \item The solution relation $S$ is decidable by a deterministic polynomial time Turing machine and is polynomially bounded (that is, the length of $y$ is bounded by a polynomial in the length of $x$ for all $(x, y)\in S$).
  \item The measure function $m$ is computable by a deterministic polynomial time Turing machine.
  \end{enumerate}
\end{definition}

The second condition is the most important in this definition; it is the analog of polynomial time verifiability in \NP.
We now wish to translate this to the setting of efficient and highly parallel verifiability.
First we define the necessary circuit classes, then we define classes of approximation problems for inherently sequential optimization problems.

\begin{definition}
  \mbox{}
  \begin{enumerate}
  \item \NC{} is the class of decision problems decidable by a uniform family of Boolean circuits with polynomial size, polylogarithmic depth, and fan-in two.
  \item \FNC{} is the class of functions computable by an \NC{} circuit in which the output of the circuit is (the binary encoding of) $f(x)$.
  \item $\NNC(f(n))$ is the class of languages computable by an \NC{} circuit family augmented with $O(f(n))$ nondeterministic gates for each input length $n$ \cite{wolf94}.
    A nondeterministic gate takes no inputs and yields a single (nondeterministic) output bit.

    If $\mathcal{F}$ is a class of functions, then $\NNC(\mathcal{F})=\bigcup_{f\in\mathcal{F}}{\NNC(f(n))}$.
  \end{enumerate}
\end{definition}

\NNCpoly, also known as \GC(\poly, \NC) \cite{cc97} and $\beta\P$ \cite{kf80}, is an unusual class which may warrant some further explanation.
\NC{} has the same relationship to \NNCpoly{} as \P{} does to \NP{} (thus an equivalent definition of \NNCpoly{} is one in which each language has an efficient and highly parallel verification procedure; as in the definition of \NPO{} in \autoref{def:npo}, it is this formulation which we use when defining \NNCO{} in \autoref{def:nnco}).
Wolf \cite{wolf94} notes that $\NNC(\log n)=\NC$ and $\NNC(\poly)=\NP$, and suggests that \NNC(\polylog) may be an interesting intermediary class, possibly incomparable with \P.
Cai and Chen \cite{cc97} prove that for each natural number $k$ and $i$, there is a complete problem for $\NNC^k(\log^i n)$ under logarithmic space reductions.

\begin{definition}\label{def:nnco}
  The complexity class \NNCOpoly{} is the class of all optimization problems $(I, S, m, t)$ such that the following conditions hold.
  \begin{enumerate}
  \item The instance set $I$ is decidable by an \NC{} circuit family.
  \item The solution relation $S$ is decidable by an \NC{} circuit family and is polynomially bounded (that is, the length of $y$ is bounded by a polynomial in the length of $x$ for all $(x, y)\in S$).
  \item The measure function $m$ is computable by an \FNC{} circuit family.
  \end{enumerate}
  For the sake of brevity, we write \NNCO{} instead of \NNCOpoly{}.
\end{definition}

Since $\NC\subseteq\P$, the inclusion $\NNCO\subseteq\NPO$ follows immediately from the definitions.
However, it is not immediately obvious from these definitions that the converse is true, since membership in \P{} does not imply membership in \NC.
The original proof of \cite{wolf94} that $\NP\subseteq\NNCpoly$ uses a Cook-Levin simulation of a Turing machine (though the proof omits the details about the polylogarithmic depth of the simulating circuit), but that technique does not extend to a proof that $\NPO\subseteq\NNCO$ since the instance set $I$ and the measure function $m$ are computable in (deterministic) polynomial time and require a (deterministic) \NC{} circuit family.

\begin{openquestion}
  Does $\NNCO=\NPO$?
\end{openquestion}

With the definitions of approximating functions and approximation schemes we can now move on to define classes of approximable optimization problems contained in \NNCO.

\begin{definition}
  Let $P$ be an optimization problem, let $r\colon \mathbb{N}\to\mathbb{N}$, and let $f\colon I\to \Sigma^*$.
  We say $f$ is an \emph{$r$-approximator for $P$} if it produces solutions for $P$ and $R(x, f(x)) \leq r(|x|)$ for all $x\in P_\exists$.

  If $r$ is the constant function with value $\delta$, we simply say $f$ is a \emph{$\delta$-approximator for $P$}.
\end{definition}

\begin{definition}
  Let $P$ be an optimization problem and let $f\colon I\times\mathbb{N}\to\Sigma^*$.
  We say $f$ is an \emph{approximation scheme for $P$} if for all $x\in P_\exists$ and all positive integers $k$ we have $f(x, k)\in S(x)$ and $R(x, f(x, k)) \leq 1 + 1 / k$.
\end{definition}

\begin{definition}
  Let $P\in\NNCO$.
  \begin{enumerate}
  \item $P\in\expApxNCO$ if there exists an $r$-approximator in \FNC{} for $P$, where $r(n)=2^{n^{O(1)}}$ for all $n\in\mathbb{N}$.
  \item $P\in\polyApxNCO$ if there exists an $r$-approximator in \FNC{} for $P$, where $r(n)=n^{O(1)}$ for all $n\in\mathbb{N}$.
  \item $P\in\logApxNCO$ if there exists an $r$-approxiator in \FNC{} for $P$, where $r(n)=O(\log n)$ for all $n\in\mathbb{N}$.
  \item $P\in\ApxNCO$ if there exists an $r$-approximator in \FNC{} for $P$, where $r(n)=O(1)$ for all $n\in\mathbb{N}$.
    (This class is also known as \NCX.)
  \item $P\in\NCAS$ if there exists an approximation scheme $f$ for $P$ such that $f_k\in\FNC$ for each $k\in\mathbb{N}$, where $f_k(x)=f(x, k)$ for all $x\in\Sigma^*$.
  \item $P\in\FNCAS$ if there exists an approximation scheme $f$ for $P$ such that $f\in\FNC$ in the sense that the size of the circuit is polynomial in both $|x|$ and $k$ and the depth of the circuit is polylogarithmic in both $|x|$ and $k$.
  \item $P\in\NCO$ if there exists a function $f$ in \FNC{} that produces optimal solutions for $P$.
  \end{enumerate}
\end{definition}

Each of these classes includes the one defined below it.

Each class also has a ``polynomially bounded'' variant, denoted by the subscript \pb, in which an optimization problem with measure $m$ and solution set $S$ has the property that there exists a polynomial $p$ such that $m(x, y) \leq p(|x|)$ for all $(x, y)\in S$.
Another common variant of these classes is the probabilistic versions of each (for example, randomized \NCAS{}, denoted \RNCAS), but we do not explore such classes in this work.

\section{Reductions among approximation problems}

There are many reductions for approximation problems; nine of them are defined in a survey paper by Crescenzi \cite{crescenzi97}.
%Although the authors of \cite{ckst95} consider it to be too restrictive for polynomial time approximation classes, we will use the \emph{error-preserving reduction} (E-reduction).
%We bound the E-reduction to logarithmic space instead of requiring that it be a \FNC{} circuit family because 1. the former notion of reduction is more restrictive and hence implies the latter, 2. existing results on approximability and approximation classes use the former, and 3. it eases analysis in some proofs.
We will use the ``AP reduction'', considered by experts to be the most reasonable reduction \cite[Section~8.6]{acgkmp99}, to compare approximation problems.

\begin{todo}
  add definition of AP reduction here
\end{todo}

We bound the AP reduction to logarithmic space instead of allowing it be a \FNC{} circuit family because 1. the former notion of reduction is more restrictive and hence implies the latter, 2. existing results on approximability and approximation classes use the former, and 3. it eases analysis in some proofs.

%% \begin{definition}
%%   let $P$ and $Q$ be optimization problems.
%%   We write $P\Er Q$ if there exists a triple $(f, g, \alpha)$, where $f$ and $g$ are functions computable in logarithmic space and $\alpha\in\mathbb{N}$, such that for all $x\in P_\exists$ we have $f(x)\in Q_\exists$ and for all $y\in S_Q(f(x))$ we have $g(x, y)\in S_P(x)$ and $R_P(x, g(x, y)) - 1 \leq \alpha \cdot (R_Q(f(x), y) - 1)$.
%% \end{definition}

\begin{proposition}
  Let $\mathcal{C}$ be one of the complexity classes \NCO, \NCAS, \ApxNCO, \logApxNCO, \polyApxNCO, \expApxNCO, or \NNCO.
  Suppose $P\in\NNCO$ and $Q\in\mathcal{C}$.
  If $P\APr Q$ then $P\in \mathcal{C}$.
  In other words, $\mathcal{C}$ is closed under $\APr$ reductions.
\end{proposition}
\begin{proof}
  \begin{todo}
    fill me in
  \end{todo}
\end{proof}

\section{Complexity of extant parallel approximations}

For a listing of somewhat contemporary results on efficient \emph{sequential} approximation, see \cite{compendium}.

\begin{proposition}
  \mbox{}
  \begin{itemize}
  \item \NNCO{} contains \nncoproblems.
  \item \expApxNCO{} contains \expapxncoproblems.
  \item \polyApxNCO{} contains \polyapxncoproblems.
  \item \logApxNCO{} contains \logapxncoproblems
  \item \ApxNCO{} contains \apxncoproblems.
  \item \NCAS{} contains \ncasproblems.
  \item \FNCAS{} contains \fncasproblems.
  \item \NCO{} contains \ncoproblems.
  \end{itemize}
\end{proposition}

Some problems exhibit ``threshold'' behavior: for certain values of a parameter (for example, $0 < \epsilon < 1/2$) the approximation problem is computable in \NC, but for other values (for example, $1/2 \leq \epsilon < 1$) the approximation problem is \P-complete (or \NP-complete).

\section{Completeness for classes of problems admitting parallel approximations}
\label{sec:completeness}

\begin{definition}
  Let $\mathcal{C}$ be one of the complexity classes \NCO, \NCAS, \ApxNCO, \logApxNCO, \polyApxNCO, \expApxNCO, or \NNCO.
  An optimization problem $Q$ is \emph{$\mathcal{C}$-hard} if for all $P\in\mathcal{C}$, we have $P\APr Q$.
  If furthermore $Q\in\mathcal{C}$, we say it is $\mathcal{C}$-complete.
\end{definition}

We wish to demonstrate complete problems for each of these approximation classes.
%To do this we start with \NP-complete optimization problems.

\begin{conjecture}
  \textsc{Maximum Weighted Satisfiability} is \NNCO-complete.
\end{conjecture}
\begin{justification}
  \textsc{Maximum Weighted Satisfiability} is \NPO-complete \cite{ckst95} and $\NNCO\subseteq\NPO$.
\end{justification}

\begin{conjecture}
  \textsc{Minimum Traveling Salesperson Problem} is complete for \expApxNCO{} under $\APr$ reductions.
\end{conjecture}
\begin{justification}
  \textsc{Minimum Traveling Salesperson Problem} is complete for \expApxPO{} under ``MPTAS reductions'' \cite[Corollary~1]{ep06}.
\end{justification}

\begin{conjecture}
  \textsc{Maximum Clique} is \polyApxNCO-complete.
\end{conjecture}
\begin{justification}
  \textsc{Maximum Clique} is complete for \polyApxPO{} under ``PTAS reductions'' \cite[Example~2.48]{cks01} \cite{kmsv99} \cite{ep10}.
\end{justification}

\begin{conjecture}
  \textsc{Minimum Set Cover} is \logApxNCO-complete.
\end{conjecture}
\begin{justification}
  \textsc{Minimum Set Cover} is complete for \logApxPO{} under ``MPTAS reductions'' \cite[Example~2.48]{cks01} \cite[Theorem~5]{ep06} \cite[Theorem~27]{ep10}.

  Also, \textsc{Minimum Set Cover} is complete for $\logApxPO_{pb}$ under ``E reductions''.
\end{justification}

In \cite{sx95}, the authors prove that \textsc{Maximum Bounded Weighted Satisfiability} is complete for \ApxNCO{} (there called ``\NCX'') under a less restrictive type of reduction called ``\NCAS{} reduction''.
Their proof adapts the technique of \cite{cp91} for constructing a canonical complete problem for \ApxPO{} (there called ``\APX'').
We translate these results into the vocabulary of this paper and show that this technique works also for E-reductions.
\begin{theorem}
  \textsc{Maximum Bounded Weighted Satisfiability} is \ApxNCO-complete.
\end{theorem}
\begin{proof}
  \begin{todo}
    fill me in by following the proof from \cite{sx95}; unfortunately, their proof is very sketchy.
    Instead, I need to find a copy of \cite{acgkmp99} which should have a clearer proof of \ApxPO-completeness (Ausiello has not posted Chapter 8, which includes the proof, on the Web).
  \end{todo}
\end{proof}

\begin{conjecture}
  \mbox{}
  \begin{enumerate}
  \item \textsc{Planar Maximum Satisfiability} is \NCAS-complete.
  \item \textsc{Planar Maximum Independent Set} is \NCAS-complete.
  \end{enumerate}
\end{conjecture}
\begin{justification}
  \mbox{}
  \begin{enumerate}
  \item \textsc{Planar Maximum Satisfiability} is in \PMPSAT, which is a subset of \PTAS{} and \emph{may} be a syntactic characterization of \PTAS{} \cite{km96}.
  \item \textsc{Planar Maximum Independent Set} is complete for \PTAS{} under ``FT reductions'' \cite[Theorem~6]{bep06}.
  \end{enumerate}
\end{justification}

\begin{conjecture}
  \textsc{0-1 Knapsack} is \FNCAS-complete.
\end{conjecture}
\begin{justification}
  \textsc{0-1 Knapsack} is in $\FPTAS\setminus\PO$ \cite[Section~3.2]{ep10} and is in \FNCAS{} \cite[Theorem~2]{mayr88}.
\end{justification}

\section{Parallel approximations for \texorpdfstring{\P}{P}-complete problems}

Most of the parallel approximation results seen in \autoref{sec:completeness} (namely, complete problems for various classes of parallel approximation problems) are based on \NP-complete problems.
While it is obviously important to provide efficient and highly parallel approximation algorithms for problems which are both intractable and inherently sequential, it is also desirable to provide highly parallel approximation algorithms for problems which are tractable but still inherently sequential.
In this section, we will describe \P-complete problems which have parallel approximations.

\begin{todo}
  fill me in.
\end{todo}

\section{Problems for which parallel approximation is \texorpdfstring{\P}{P}-complete}

In \cite[Section~10.2]{ghr95}, the authors provide a very informal statement that ``there are no \NC{} approximation algorithms'' for the following problems: \textsc{Lexicographically First Maximal Independent Set Size}, \textsc{Unit Resolution}, \textsc{Generability}, \textsc{Path Systems}, and \textsc{Circuit Value Problem}.
They further state that the following problems exhibit threshold behavior, in the sense that computing an $\epsilon$-approximation is \P-complete if $\epsilon\geq 2$ but is in \NCAS{} if $\epsilon<2$: \textsc{High Degree Subgraph} and \textsc{High Connectivity Subgraph}.

\begin{todo}
  figure out the formal results for each of these.
\end{todo}

\section{About this work}

Copyright 2012 Jef{}frey Finkelstein.

This work is licensed under the Creative Commons Attribution-ShareAlike License 3.0.
Visit \mbox{\url{https://creativecommons.org/licenses/by-sa/3.0/}} to view a copy of this license.

The \LaTeX{} markup which generated this document is available on the World Wide Web at \mbox{\url{https://github.com/jfinkels/parallel}}.
It is also licensed under the Creative Commons Attribution-ShareAlike License.

The author can be contacted via email at \email{jeffreyf@bu.edu}.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
